Here is the Project Definition Report (PDR) for enhancing the recipe OCR processing system with an LLM:

Project Definition Report: LLM Enhancement for Recipe OCR Processing

1. Introduction & Problem Statement

Current System: The application currently uses Tesseract.js for Optical Character Recognition (OCR) to extract text from recipe images. The extracted raw text is then processed by a custom-built parser (src/services/ocr/parser/) which attempts to identify and structure key recipe components (title, ingredients, instructions, prep/cook time, servings). This structured data is then normalized, validated, and used to populate the recipe form.
Problem: The current custom parser struggles with the inherent variability and potential inaccuracies of OCR output. It often fails to correctly identify, clean, and structure the text, especially for recipes with non-standard layouts or complex formatting. This leads to incomplete or incorrectly populated recipe forms, requiring significant manual correction by the user and diminishing the value of the import feature. The current parser also doesn't attempt to extract fields like dietType or mealType.
2. Proposed Solution

Introduce a Large Language Model (LLM) into the data processing pipeline between the OCR step and the data normalization/validation step.
The raw (or minimally cleaned) text extracted by the OCR service will be sent to an LLM service.
The LLM will be prompted to analyze the text, identify the relevant recipe components (title, ingredients, instructions, times, servings, etc.), clean them, and structure them according to a predefined schema that aligns with the application's recipe data model.
The structured data returned by the LLM will replace the output currently generated by the custom parseRecipeText function.
The existing normalization and validation steps (normalizeRecipe, validateRecipe) will be adapted to work with the LLM's output, ensuring consistency and handling potential LLM errors or omissions gracefully.
3. Goals

Significantly improve the accuracy and reliability of structuring recipe text extracted via OCR.
Reduce the need for manual correction by users after importing recipes from images.
Successfully parse a wider variety of recipe formats and layouts.
Potentially infer additional fields like dietType and mealType based on the recipe content (ingredients, title).
Provide a more robust and flexible foundation for future enhancements to the recipe import feature.
4. Scope

In Scope:
Integrating an external LLM API (e.g., OpenAI, Google Gemini, Anthropic Claude) or a locally hosted LLM service.
Developing and refining prompts for the LLM to achieve optimal recipe parsing.
Replacing the existing parseRecipeText function call within ocrService.js with a call to the new LLM processing logic.
Modifying the cleanRecipeText function in ocrService.js to orchestrate the LLM call.
Adapting the normalizeRecipe and validateRecipe functions to handle the LLM output format and potential variations.
Implementing error handling for LLM API calls (e.g., timeouts, rate limits, invalid responses).
Basic logging and monitoring for the LLM interaction.
Out of Scope:
Changing the underlying OCR engine (Tesseract.js).
Modifying the recipe form UI (RecipeForm.js and related components).
Making significant changes to the core recipe data model stored in Firestore.
Building a custom LLM from scratch.
Advanced features like multi-language support beyond English (unless the chosen LLM inherently supports it well).
5. High-Level Design

The proposed data flow will be:

Current System (To Be Replaced)

Replaces

Structured JSON - (The data should be compatible with our current structure)

Image Input

OCR Service: extractRawTextFromImage

Clean/Structure Text

LLM Service Call

OCR Service: normalizeRecipe

OCR Service: validateRecipe

Recipe Form Population

Custom Parser: parseRecipeText

ocrService.js (cleanRecipeText): Will be modified. Instead of calling the local parseRecipeText, it will:
Prepare the raw OCR text.
Construct the appropriate prompt for the LLM.
Call the chosen LLM service API with the text and prompt.
Receive the structured JSON response from the LLM.
Perform basic validation/parsing of the LLM response.
Return the structured data.
LLM Service Wrapper (New Module, e.g., src/services/llmService.js): A new module could encapsulate the logic for interacting with the chosen LLM API, handling authentication, request formatting, and response parsing.
Normalization/Validation (ocrService.js): normalizeRecipe and validateRecipe will be reviewed and potentially updated to align perfectly with the expected LLM output structure and handle cases where the LLM might fail to extract certain fields.
6. Technical Considerations

LLM Choice: Evaluate options like OpenAI (GPT-4/GPT-3.5), Google Gemini, Anthropic Claude, or potentially locally hosted models (e.g., via Ollama) based on accuracy, cost, latency, and ease of integration.
API Integration: Securely manage API keys and handle network requests/responses.
Prompt Engineering: This will be critical. Iteratively develop and test prompts that instruct the LLM to extract the desired fields accurately, handle variations in input text, and return data in the specified JSON format. Consider techniques like few-shot prompting if needed.
Cost: LLM APIs have associated costs per token. Estimate usage and budget accordingly.
Latency: LLM API calls will introduce latency compared to the local parser. Assess the impact on user experience and consider optimizations (e.g., asynchronous processing, streaming responses if applicable).
Error Handling: Implement robust error handling for API failures, rate limits, malformed LLM responses, or cases where the LLM cannot confidently parse the recipe. Provide fallback mechanisms or clear error messages to the user.
Data Privacy: Review the terms of service of the chosen LLM provider regarding data usage and privacy, especially since recipe text might occasionally contain personal notes.
7. Data Model (Target LLM Output)

The LLM should be prompted to return a JSON object closely matching the structure used by normalizeRecipe:

{
  "title": "String",
  "ingredients": [
    "String Ingredient 1",
    "String Ingredient 2",
    // ...
  ],
  "instructions": "String (potentially multi-line)",
  "prepTime": "String (e.g., '15 minutes')",
  "cookTime": "String (e.g., '30 minutes')",
  "servings": "String (e.g., '4 servings')",
  "dietType": "String (e.g., 'Vegetarian', 'Gluten-Free', or empty)",
  "mealType": "String (e.g., 'Dinner', 'Dessert', or empty)"
}
(Note: The exact format, especially for ingredients, might need refinement based on how normalizeRecipe and the form expect them).

8. Success Metrics

Reduction in Manual Edits: Track the frequency of users editing fields immediately after an OCR import. Aim for a significant reduction (e.g., >50%).
Successful Import Rate: Measure the percentage of image imports that result in a reasonably complete and accurate form population without critical errors.
User Feedback: Collect qualitative feedback from users regarding the improved import experience.
Field Coverage: Track the percentage of imports where the LLM successfully extracts each key field (title, ingredients, instructions, time, servings) and potentially dietType/mealType.
9. Risks

LLM Accuracy/Consistency: LLMs can sometimes "hallucinate" or provide inconsistent results. Requires careful prompt tuning and validation.
Cost Overruns: Higher-than-expected usage or choosing a more expensive model could impact budget.
Increased Latency: API calls will slow down the import process. May require UI adjustments to manage user expectations (e.g., loading indicators).
Prompt Brittleness: Prompts might need ongoing adjustments if the LLM provider updates their models.
API Availability/Rate Limits: Dependence on an external service introduces potential downtime or throttling.
Handling Edge Cases: Recipes with very unusual formatting or poor OCR quality might still pose challenges.
10. Next Steps

LLM Selection & Setup: Choose an LLM provider/model, obtain API keys, and set up a basic client/wrapper (llmService.js).
Initial Prompt Development: Create a first version of the prompt for recipe extraction.
Integration: Modify ocrService.js to call the LLM service instead of parseRecipeText.
Testing & Refinement: Test with a diverse set of recipe images, analyze LLM outputs, and iteratively refine the prompt and parsing logic in ocrService.js.
Adapt Normalization/Validation: Update normalizeRecipe and validateRecipe as needed.
Error Handling Implementation: Build robust error handling around the LLM call.
User Interface: Consider adding loading indicators or feedback during the LLM processing step.
Deployment & Monitoring: Deploy the changes and monitor performance, cost, and error rates.